import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import warnings
warnings.filterwarnings("ignore")

import whisperx
import torch
from typing import Dict
import librosa
from rich import print as rprint
import subprocess
import tempfile
import time

from core.config_utils import load_key
from core.all_whisper_methods.demucs_vl import demucs_main, RAW_AUDIO_FILE, VOCAL_AUDIO_FILE
from core.all_whisper_methods.whisperX_utils import process_transcription, convert_video_to_audio, split_audio, save_results, save_language, compress_audio, CLEANED_CHUNKS_EXCEL_PATH
from core.step1_ytdlp import find_video_files

# å®šä¹‰å¸¸é‡è·¯å¾„
MODEL_DIR = load_key("model_dir")  # æ¨¡å‹å­˜å‚¨ç›®å½•
WHISPER_FILE = "output/audio/for_whisper.mp3"  # Whisperè¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
ENHANCED_VOCAL_PATH = "output/audio/enhanced_vocals.mp3"  # å¢å¼ºåçš„äººå£°æ–‡ä»¶è·¯å¾„

def check_hf_mirror() -> str:
    """æ£€æŸ¥å¹¶è¿”å›æœ€å¿«çš„HuggingFaceé•œåƒç«™ç‚¹"""
    mirrors = {
        'Official': 'huggingface.co',  # å®˜æ–¹é•œåƒ
        'Mirror': 'hf-mirror.com'      # å¤‡ç”¨é•œåƒ
    }
    fastest_url = f"https://{mirrors['Official']}"
    best_time = float('inf')
    rprint("[cyan]ğŸ” Checking HuggingFace mirrors...[/cyan]")
    for name, domain in mirrors.items():
        try:
            if os.name == 'nt':
                cmd = ['ping', '-n', '1', '-w', '3000', domain]
            else:
                cmd = ['ping', '-c', '1', '-W', '3', domain]
            start = time.time()
            result = subprocess.run(cmd, capture_output=True, text=True)
            response_time = time.time() - start
            if result.returncode == 0:
                if response_time < best_time:
                    best_time = response_time
                    fastest_url = f"https://{domain}"
                rprint(f"[green]âœ“ {name}:[/green] {response_time:.2f}s")
        except:
            rprint(f"[red]âœ— {name}:[/red] Failed to connect")
    if best_time == float('inf'):
        rprint("[yellow]âš ï¸ All mirrors failed, using default[/yellow]")
    rprint(f"[cyan]ğŸš€ Selected mirror:[/cyan] {fastest_url} ({best_time:.2f}s)")
    return fastest_url

def transcribe_audio(audio_file: str, start: float, end: float) -> Dict:
    """
    ä½¿ç”¨WhisperXè½¬å½•éŸ³é¢‘ç‰‡æ®µ
    
    å‚æ•°:
        audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        start: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰
        end: ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰
    è¿”å›:
        åŒ…å«è½¬å½•ç»“æœçš„å­—å…¸
    """
    os.environ['HF_ENDPOINT'] = check_hf_mirror() #? don't know if it's working...
    WHISPER_LANGUAGE = load_key("whisper.language")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    rprint(f"ğŸš€ Starting WhisperX using device: {device} ...")
    
    if device == "cuda":
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        batch_size = 16 if gpu_mem > 8 else 2
        compute_type = "float16" if torch.cuda.is_bf16_supported() else "int8"
        rprint(f"[cyan]ğŸ® GPU memory:[/cyan] {gpu_mem:.2f} GB, [cyan]ğŸ“¦ Batch size:[/cyan] {batch_size}, [cyan]âš™ï¸ Compute type:[/cyan] {compute_type}")
    else:
        batch_size = 1
        compute_type = "int8"
        rprint(f"[cyan]ğŸ“¦ Batch size:[/cyan] {batch_size}, [cyan]âš™ï¸ Compute type:[/cyan] {compute_type}")
    rprint(f"[green]â–¶ï¸ Starting WhisperX for segment {start:.2f}s to {end:.2f}s...[/green]")
    
    try:
        if WHISPER_LANGUAGE == 'zh':
            model_name = "Huan69/Belle-whisper-large-v3-zh-punct-fasterwhisper"
            local_model = os.path.join(MODEL_DIR, "Belle-whisper-large-v3-zh-punct-fasterwhisper")
        else:
            model_name = load_key("whisper.model")
            local_model = os.path.join(MODEL_DIR, model_name)
            
        if os.path.exists(local_model):
            rprint(f"[green]ğŸ“¥ Loading local WHISPER model:[/green] {local_model} ...")
            model_name = local_model
        else:
            rprint(f"[green]ğŸ“¥ Using WHISPER model from HuggingFace:[/green] {model_name} ...")

        vad_options = {"vad_onset": 0.500,"vad_offset": 0.363}
        asr_options = {"temperatures": [0],"initial_prompt": "",}
        whisper_language = None if 'auto' in WHISPER_LANGUAGE else WHISPER_LANGUAGE
        rprint("[bold yellow]**You can ignore warning of `Model was trained with torch 1.10.0+cu102, yours is 2.0.0+cu118...`**[/bold yellow]")
        model = whisperx.load_model(model_name, device, compute_type=compute_type, language=whisper_language, vad_options=vad_options, asr_options=asr_options, download_root=MODEL_DIR)

        # Create temp file with wav format for better compatibility
        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_audio:
            temp_audio_path = temp_audio.name
        
        # Extract audio segment using ffmpeg
        ffmpeg_cmd = f'ffmpeg -y -i "{audio_file}" -ss {start} -t {end-start} -vn -ar 32000 -ac 1 "{temp_audio_path}"'
        subprocess.run(ffmpeg_cmd, shell=True, check=True, capture_output=True)
        
        try:
            # Load audio segment with librosa
            audio_segment, sample_rate = librosa.load(temp_audio_path, sr=16000)
        finally:
            # Clean up temp file
            if os.path.exists(temp_audio_path):
                os.unlink(temp_audio_path)

        rprint("[bold green]note: You will see Progress if working correctly[/bold green]")
        result = model.transcribe(audio_segment, batch_size=batch_size, print_progress=True)

        # Free GPU resources
        del model
        torch.cuda.empty_cache()

        # Save language
        save_language(result['language'])
        if result['language'] == 'zh' and WHISPER_LANGUAGE != 'zh':
            raise ValueError("Please specify the transcription language as zh and try again!")

        # Align whisper output
        model_a, metadata = whisperx.load_align_model(language_code=result["language"], device=device)
        result = whisperx.align(result["segments"], model_a, metadata, audio_segment, device, return_char_alignments=False)

        # Free GPU resources again
        torch.cuda.empty_cache()
        del model_a

        # Adjust timestamps
        for segment in result['segments']:
            segment['start'] += start
            segment['end'] += start
            for word in segment['words']:
                if 'start' in word:
                    word['start'] += start
                if 'end' in word:
                    word['end'] += start
        return result
    except Exception as e:
        rprint(f"[red]WhisperX processing error:[/red] {e}")
        raise

def enhance_vocals(vocals_ratio=2.50):
    """
    å¢å¼ºäººå£°éŸ³é‡
    
    å‚æ•°:
        vocals_ratio: éŸ³é‡å¢ç›Šæ¯”ä¾‹ï¼Œé»˜è®¤2.50
    è¿”å›:
        å¤„ç†åçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    """
    if not load_key("demucs"):
        return RAW_AUDIO_FILE
        
    try:
        print(f"[cyan]ğŸ™ï¸ Enhancing vocals with volume ratio: {vocals_ratio}[/cyan]")
        ffmpeg_cmd = (
            f'ffmpeg -y -i "{VOCAL_AUDIO_FILE}" '
            f'-filter:a "volume={vocals_ratio}" '
            f'"{ENHANCED_VOCAL_PATH}"'
        )
        subprocess.run(ffmpeg_cmd, shell=True, check=True, capture_output=True)
        
        return ENHANCED_VOCAL_PATH
    except subprocess.CalledProcessError as e:
        print(f"[red]Error enhancing vocals: {str(e)}[/red]")
        return VOCAL_AUDIO_FILE  # Fallback to original vocals if enhancement fails
    
def transcribe():
    """
    ä¸»è½¬å½•æµç¨‹å‡½æ•°ï¼ŒåŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š
    1. è§†é¢‘è½¬éŸ³é¢‘
    2. äººå£°åˆ†ç¦»ï¼ˆå¯é€‰ï¼‰
    3. éŸ³é¢‘å‹ç¼©
    4. éŸ³é¢‘åˆ†æ®µ
    5. è½¬å½•å¤„ç†
    6. ç»“æœåˆå¹¶ä¸ä¿å­˜
    """
    if os.path.exists(CLEANED_CHUNKS_EXCEL_PATH):
        rprint("[yellow]âš ï¸ è½¬å½•ç»“æœå·²å­˜åœ¨ï¼Œè·³è¿‡è½¬å½•æ­¥éª¤ã€‚[/yellow]")
        return
    
    # æ­¥éª¤0ï¼šè§†é¢‘è½¬éŸ³é¢‘
    video_file = find_video_files()
    convert_video_to_audio(video_file)

    # æ­¥éª¤1ï¼šä½¿ç”¨Demucsè¿›è¡Œäººå£°åˆ†ç¦»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if load_key("demucs"):
        demucs_main()
    
    # æ­¥éª¤2ï¼šå‹ç¼©éŸ³é¢‘
    choose_audio = enhance_vocals() if load_key("demucs") else RAW_AUDIO_FILE
    whisper_audio = compress_audio(choose_audio, WHISPER_FILE)

    # æ­¥éª¤3ï¼šåˆ†å‰²éŸ³é¢‘
    segments = split_audio(whisper_audio)
    
    # æ­¥éª¤4ï¼šè½¬å½•éŸ³é¢‘ç‰‡æ®µ
    all_results = []
    for start, end in segments:
        result = transcribe_audio(whisper_audio, start, end)
        all_results.append(result)
    
    # æ­¥éª¤5ï¼šåˆå¹¶ç»“æœ
    combined_result = {'segments': []}
    for result in all_results:
        combined_result['segments'].extend(result['segments'])
    
    # æ­¥éª¤6ï¼šå¤„ç†å¹¶ä¿å­˜å¤šç§æ ¼å¼çš„ç»“æœ
    # ä¿å­˜Excelæ ¼å¼ï¼ˆåŸæœ‰åŠŸèƒ½ï¼‰
    df = process_transcription(combined_result)
    save_results(df)
    
    # ä¿å­˜çº¯æ–‡æœ¬æ ¼å¼
    output_txt = "output/transcript.txt"
    with open(output_txt, 'w', encoding='utf-8') as f:
        for segment in combined_result['segments']:
            f.write(segment['text'] + '\n')
    rprint(f"[green]âœ“ Saved transcript to:[/green] {output_txt}")
    
    # ä¿å­˜SRTæ ¼å¼
    output_srt = "output/transcript.srt"
    with open(output_srt, 'w', encoding='utf-8') as f:
        for i, segment in enumerate(combined_result['segments'], 1):
            # SRTæ ¼å¼ï¼šåºå· + æ—¶é—´ç  + æ–‡æœ¬ + ç©ºè¡Œ
            start = format_timestamp(segment['start'])
            end = format_timestamp(segment['end'])
            f.write(f"{i}\n")
            f.write(f"{start} --> {end}\n")
            f.write(f"{segment['text']}\n\n")
    rprint(f"[green]âœ“ Saved SRT to:[/green] {output_srt}")

def format_timestamp(seconds: float) -> str:
    """
    å°†ç§’æ•°è½¬æ¢ä¸ºSRTæ ¼å¼çš„æ—¶é—´æˆ³ (HH:MM:SS,mmm)
    """
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds = seconds % 60
    milliseconds = int((seconds % 1) * 1000)
    seconds = int(seconds)
    return f"{hours:02d}:{minutes:02d}:{seconds:02d},{milliseconds:03d}"

if __name__ == "__main__":
    transcribe()
